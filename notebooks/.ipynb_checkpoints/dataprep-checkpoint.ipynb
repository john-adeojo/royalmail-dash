{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f66ef2f-6d5e-4b3e-8337-ddf8cd427146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f10088-ad74-4969-80d8-512dcd9d0450",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "project_dir = os.path.dirname(notebook_dir)\n",
    "\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afc1c74-6ca5-496e-9aa1-099e6b3135d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv(r\"C:\\Users\\johna\\anaconda3\\envs\\royalmail-dash\\royalmail-dash\\data\\01_raw\\tweets_details2023-03-15_20-43-36.csv\")\n",
    "tweets_df = tweets_df.loc[tweets_df['language'] == 'en'].copy()\n",
    "tweets_df = tweets_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0edbe3-1410-4e22-a784-479e84895db0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from scripts.data_processing.preprocess_tweets_lite import TextCleaner\n",
    "\n",
    "\n",
    "def get_topics(df, MODEL, model_path):\n",
    "    \n",
    "    _df = df.copy()\n",
    "        \n",
    "\n",
    "    # load model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "    # Set up tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "    cleaner = TextCleaner(stop_words_remove=False)\n",
    "\n",
    "    _df['cleaned_text'] = _df['text'].apply(cleaner.clean_text)\n",
    "\n",
    "    # set up pipeline\n",
    "    classifier = pipeline(\n",
    "        \"text-classification\", model=model, tokenizer=tokenizer, device=0\n",
    "    )\n",
    "\n",
    "    # perform inference and extract predicted class and probability\n",
    "    results = classifier(list(_df[\"cleaned_text\"]), truncation=True)\n",
    "    \n",
    "    _df[\"topic\"] = [result['label'] for result in results]\n",
    "    _df[\"topic_score\"] = [result['score'] for result in results]\n",
    "    \n",
    "    # Create a new DataFrame with the required columns\n",
    "    output_df = _df[[\"topic\", \"topic_score\", \"cleaned_text\", \"user.location\", \"user.is_blue_verified\", \"favorite_count\", \"retweet_count\", \"reply_count\", \"quote_count\", \"creation_date\"]]\n",
    "\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a4c3e8-3a1a-4cdf-9f42-b98a94cfd401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from scripts.data_processing.preprocess_tweets_lite import TextCleaner\n",
    "\n",
    "\n",
    "def get_emotion(df):\n",
    "    \n",
    "    _df = df.copy()\n",
    "            \n",
    "    task='emotion'\n",
    "    MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"    \n",
    "\n",
    "    # load model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "    # Set up tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "    cleaner = TextCleaner(stop_words_remove=False)\n",
    "\n",
    "    # set up pipeline\n",
    "    classifier = pipeline(\n",
    "        \"text-classification\", model=model, tokenizer=tokenizer, device=0\n",
    "    )\n",
    "\n",
    "    # perform inference and extract predicted class and probability\n",
    "    results = classifier(list(_df[\"cleaned_text\"]), truncation=True)\n",
    "    \n",
    "    _df[\"emotion\"] = [result['label'] for result in results]\n",
    "    _df[\"emotion_score\"] = [result['score'] for result in results]\n",
    "    \n",
    "    return _df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8fa198-5b5d-464c-b3c8-010acea01324",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "\n",
    "def get_lat_lon(location):\n",
    "    geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "    try:\n",
    "        location_data = geolocator.geocode(location, timeout=10)\n",
    "        if location_data:\n",
    "            return location_data.latitude, location_data.longitude\n",
    "    except GeocoderTimedOut:\n",
    "        return get_lat_lon(location)\n",
    "    return None, None\n",
    "\n",
    "def add_lat_lon_columns(df, location_column):\n",
    "    df_copy = df.copy()\n",
    "    df_copy['coordinates'] = df_copy[location_column].apply(get_lat_lon)\n",
    "    df_copy[['latitude', 'longitude']] = pd.DataFrame(df_copy['coordinates'].tolist(), index=df_copy.index)\n",
    "    df_copy = df_copy.drop(columns=['coordinates'])\n",
    "    return df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99fcf8a-c007-4234-bbf5-0c9d4b28d2b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets_dash_data = get_topics(df=tweets_df, MODEL=\"cardiffnlp/twitter-roberta-base\", model_path=r\"C:\\Users\\johna\\OneDrive\\Desktop\\models_twitter_dash\\output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97ac582-98dc-4535-9b3c-f8b9b4762745",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_dash_data_2 = get_emotion(df=tweets_dash_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1052f5a9-605a-43c5-8218-12c1020be21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_dash_final = add_lat_lon_columns(tweets_dash_data_2, 'user.location')\n",
    "tweets_dash_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3163e3-5ceb-46a3-ba6b-fd04d5407df8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tweets_dash_final.to_csv(r\"C:\\Users\\johna\\anaconda3\\envs\\royalmail-dash\\royalmail-dash\\data\\02_intermediate\\tweets_dash.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
